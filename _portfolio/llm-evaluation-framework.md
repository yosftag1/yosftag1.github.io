---
title: "Arabic LLM Evaluation Framework"
excerpt: "A comprehensive rubric-based evaluation framework for assessing Arabic Large Language Models in educational conversational applications"
collection: portfolio
header:
  teaser: "projects/llm-evaluation/radar-models-full.png"

---

## Project Overview

This project develops a **systematic evaluation framework** for Arabic Large Language Models (LLMs) specifically designed for educational conversational applications. The work addresses a critical gap in Arabic NLP research and is currently under review at **Language Resources and Evaluation (Q1 journal)**.

## Technical Details & Achievements

| Aspect | Details |
|--------|---------|
| **Focus** | Educational conversational AI |
| **Language** | Arabic |
| **Methods** | Rubric-based qualitative + quantitative metrics |
| **Achievement** | Developed novel comparative framework for Arabic LLMs |
| **Status** | Under Review at **Language Resources and Evaluation (Q1)** |
| **Role** | Coordinated team, led framework design, experimental setup, and analysis |

## The Challenge

Existing LLM benchmarks focus primarily on English and use quantitative metrics that miss nuanced aspects of educational value. LLMs in genreal and specially Arabic LLMs need evaluation that captures the core purpose of the LLMs use cause in this case educational conversational applications:
- Pedagogical effectiveness
- Cultural and age appropriateness
- Conversational quality in educational contexts

## Framework Approach

Our framework introduces a **rubric-based evaluation system** that enables qualitative and holistic assessment:

<div style="display: flex; gap: 20px; align-items: center;">
  <div style="flex: 40%;">
    <a href="/images/projects/llm-evaluation/radar-models-full.png">
      <img src="/images/projects/llm-evaluation/radar-models-full.png" alt="LLM Comparison Radar Charts">
    </a>
  </div>
  <div style="flex: 60%;">
    <a href="/images/projects/llm-evaluation/model-vs-method.png">
      <img src="/images/projects/llm-evaluation/model-vs-method.png" alt="Model vs Method Analysis">
    </a>
  </div>
</div>
<p style="text-align: center; font-style: italic; font-size: 0.85em; color: #555;">Left: Comparative evaluation of Arabic LLMs. Right: Analysis comparing different evaluation methods.</p>

## Key Contributions

- **Novel Evaluation Rubric**: Multi-dimensional assessment beyond simple accuracy metrics
- **Arabic-Specific Focus**: Addresses unique challenges of Arabic language processing in education
- **Inter-Rater Reliability**: Robust validation ensuring consistent evaluation across evaluators
- **Comparative Framework**: Systematic comparison of multiple Arabic LLMs

## Inter-Rater Reliability Analysis

![Inter-Rater Metrics](/images/projects/llm-evaluation/inter-rater-metrics.png)
*Statistical validation of evaluation consistency across human raters*



## Research Team

Led by **Dr. Shaimaa Lazem** (SRTA-City) and **Dr. Christine Basta** (Polytechnic University of Catalonia), in collaboration with undergraduate students from **Alexandria University**. My role focused on framework design, experimental setup, and analysis.

## Code Repository

[github.com/yosftag1/arabic-llm-evaluation-framework](https://github.com/yosftag1/arabic-llm-evaluation-framework)

## Publication Status

**Under Review** at Language Resources and Evaluation (Q1)  
*Preprint available upon request*
